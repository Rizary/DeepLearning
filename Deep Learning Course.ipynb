{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import gzip\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json('./file.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = df['title'].values.tolist()\n",
    "content = df['content'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ethiopia's new Prime Minister has had a stellar two months, can he keep it up? \""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = [s.lower() for s in title]\n",
    "content = [s.lower() for s in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = title+content\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_corp = [nltk.word_tokenize(sent) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 07:22:15,848 : INFO : collecting all words and their counts\n",
      "2018-06-28 07:22:15,849 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-28 07:22:15,857 : INFO : collected 3619 word types from a corpus of 15980 raw words and 64 sentences\n",
      "2018-06-28 07:22:15,863 : INFO : Loading a fresh vocabulary\n",
      "2018-06-28 07:22:15,873 : INFO : min_count=1 retains 3619 unique words (100% of original 3619, drops 0)\n",
      "2018-06-28 07:22:15,879 : INFO : min_count=1 leaves 15980 word corpus (100% of original 15980, drops 0)\n",
      "2018-06-28 07:22:15,901 : INFO : deleting the raw counts dictionary of 3619 items\n",
      "2018-06-28 07:22:15,903 : INFO : sample=0.001 downsamples 41 most-common words\n",
      "2018-06-28 07:22:15,905 : INFO : downsampling leaves estimated 11764 word corpus (73.6% of prior 15980)\n",
      "2018-06-28 07:22:15,921 : INFO : estimated required memory for 3619 words and 1000 dimensions: 30761500 bytes\n",
      "2018-06-28 07:22:15,923 : INFO : resetting layer weights\n",
      "2018-06-28 07:22:16,084 : INFO : training model with 3 workers on 3619 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-28 07:22:16,101 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:16,215 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:16,247 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:16,250 : INFO : EPOCH - 1 : training on 15980 raw words (11753 effective words) took 0.2s, 74207 effective words/s\n",
      "2018-06-28 07:22:16,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:16,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:16,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:16,408 : INFO : EPOCH - 2 : training on 15980 raw words (11748 effective words) took 0.1s, 78762 effective words/s\n",
      "2018-06-28 07:22:16,428 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:16,542 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:16,576 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:16,578 : INFO : EPOCH - 3 : training on 15980 raw words (11757 effective words) took 0.2s, 70717 effective words/s\n",
      "2018-06-28 07:22:16,595 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:16,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:16,721 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:16,723 : INFO : EPOCH - 4 : training on 15980 raw words (11779 effective words) took 0.1s, 83475 effective words/s\n",
      "2018-06-28 07:22:16,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:16,842 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:16,869 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:16,873 : INFO : EPOCH - 5 : training on 15980 raw words (11706 effective words) took 0.1s, 79302 effective words/s\n",
      "2018-06-28 07:22:16,877 : INFO : training on a 79900 raw words (58743 effective words) took 0.8s, 74715 effective words/s\n",
      "2018-06-28 07:22:16,878 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-06-28 07:22:16,880 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-06-28 07:22:16,884 : INFO : training model with 3 workers on 3619 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-28 07:22:17,032 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:17,047 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:17,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:17,062 : INFO : EPOCH - 1 : training on 84438 raw words (16629 effective words) took 0.2s, 94819 effective words/s\n",
      "2018-06-28 07:22:17,214 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:17,219 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:17,225 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:17,227 : INFO : EPOCH - 2 : training on 84438 raw words (16679 effective words) took 0.2s, 103157 effective words/s\n",
      "2018-06-28 07:22:17,366 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:17,382 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:17,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:17,392 : INFO : EPOCH - 3 : training on 84438 raw words (16659 effective words) took 0.2s, 103103 effective words/s\n",
      "2018-06-28 07:22:17,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:17,543 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:17,552 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:17,556 : INFO : EPOCH - 4 : training on 84438 raw words (16778 effective words) took 0.2s, 104651 effective words/s\n",
      "2018-06-28 07:22:17,695 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:17,705 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:17,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:17,723 : INFO : EPOCH - 5 : training on 84438 raw words (16732 effective words) took 0.2s, 102654 effective words/s\n",
      "2018-06-28 07:22:17,856 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:17,864 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:17,875 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:17,877 : INFO : EPOCH - 6 : training on 84438 raw words (16725 effective words) took 0.2s, 111182 effective words/s\n",
      "2018-06-28 07:22:18,013 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:18,017 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:18,029 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:18,032 : INFO : EPOCH - 7 : training on 84438 raw words (16672 effective words) took 0.2s, 109254 effective words/s\n",
      "2018-06-28 07:22:18,165 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:18,177 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:18,194 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:18,196 : INFO : EPOCH - 8 : training on 84438 raw words (16614 effective words) took 0.2s, 104955 effective words/s\n",
      "2018-06-28 07:22:18,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:18,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:18,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:18,355 : INFO : EPOCH - 9 : training on 84438 raw words (16771 effective words) took 0.2s, 108326 effective words/s\n",
      "2018-06-28 07:22:18,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:18,491 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:18,511 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:18,517 : INFO : EPOCH - 10 : training on 84438 raw words (16666 effective words) took 0.2s, 105338 effective words/s\n",
      "2018-06-28 07:22:18,518 : INFO : training on a 844380 raw words (166925 effective words) took 1.6s, 102214 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(166925, 844380)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(tok_corp,min_count=1,size=1000)\n",
    "model.train(corpus,total_examples=len(corpus),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 07:22:19,254 : INFO : Done creating list and to lower word\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Done creating list and to lower word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 07:22:22,701 : INFO : collecting all words and their counts\n",
      "2018-06-28 07:22:22,705 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2018-06-28 07:22:22,707 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-28 07:22:22,708 : INFO : collected 38 word types from a corpus of 2165 raw words and 32 sentences\n",
      "2018-06-28 07:22:22,717 : INFO : Loading a fresh vocabulary\n",
      "2018-06-28 07:22:22,718 : INFO : min_count=5 retains 27 unique words (71% of original 38, drops 11)\n",
      "2018-06-28 07:22:22,718 : INFO : min_count=5 leaves 2142 word corpus (98% of original 2165, drops 23)\n",
      "2018-06-28 07:22:22,720 : INFO : deleting the raw counts dictionary of 38 items\n",
      "2018-06-28 07:22:22,723 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2018-06-28 07:22:22,728 : INFO : downsampling leaves estimated 372 word corpus (17.4% of prior 2142)\n",
      "2018-06-28 07:22:22,728 : INFO : estimated required memory for 27 words and 1000 dimensions: 229500 bytes\n",
      "2018-06-28 07:22:22,729 : INFO : resetting layer weights\n",
      "2018-06-28 07:22:22,730 : INFO : training model with 5 workers on 27 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-06-28 07:22:22,746 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:22,747 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:22,749 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:22,751 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:22,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:22,754 : INFO : EPOCH - 1 : training on 2165 raw words (355 effective words) took 0.0s, 28347 effective words/s\n",
      "2018-06-28 07:22:22,764 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:22,769 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:22,772 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:22,772 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:22,778 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:22,778 : INFO : EPOCH - 2 : training on 2165 raw words (378 effective words) took 0.0s, 20965 effective words/s\n",
      "2018-06-28 07:22:22,788 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:22,789 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:22,793 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:22,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:22,797 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:22,798 : INFO : EPOCH - 3 : training on 2165 raw words (367 effective words) took 0.0s, 23590 effective words/s\n",
      "2018-06-28 07:22:22,804 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:22,810 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:22,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:22,818 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:22,818 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:22,823 : INFO : EPOCH - 4 : training on 2165 raw words (379 effective words) took 0.0s, 19055 effective words/s\n",
      "2018-06-28 07:22:22,832 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:22,835 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:22,837 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:22,841 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:22,842 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:22,844 : INFO : EPOCH - 5 : training on 2165 raw words (383 effective words) took 0.0s, 21858 effective words/s\n",
      "2018-06-28 07:22:22,844 : INFO : training on a 10825 raw words (1862 effective words) took 0.1s, 16739 effective words/s\n",
      "2018-06-28 07:22:22,847 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-06-28 07:22:22,848 : INFO : collecting all words and their counts\n",
      "2018-06-28 07:22:22,848 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2018-06-28 07:22:22,851 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-28 07:22:22,875 : INFO : collected 59 word types from a corpus of 82273 raw words and 32 sentences\n",
      "2018-06-28 07:22:22,878 : INFO : Loading a fresh vocabulary\n",
      "2018-06-28 07:22:22,878 : INFO : min_count=5 retains 49 unique words (83% of original 59, drops 10)\n",
      "2018-06-28 07:22:22,881 : INFO : min_count=5 leaves 82258 word corpus (99% of original 82273, drops 15)\n",
      "2018-06-28 07:22:22,882 : INFO : deleting the raw counts dictionary of 59 items\n",
      "2018-06-28 07:22:22,883 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2018-06-28 07:22:22,890 : INFO : downsampling leaves estimated 15584 word corpus (18.9% of prior 82258)\n",
      "2018-06-28 07:22:22,891 : INFO : estimated required memory for 49 words and 1000 dimensions: 416500 bytes\n",
      "2018-06-28 07:22:22,891 : INFO : resetting layer weights\n",
      "2018-06-28 07:22:22,898 : INFO : training model with 5 workers on 49 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-06-28 07:22:23,011 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:23,061 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:23,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:23,091 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:23,095 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:23,096 : INFO : EPOCH - 1 : training on 82273 raw words (15558 effective words) took 0.2s, 79404 effective words/s\n",
      "2018-06-28 07:22:23,204 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:23,240 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:23,258 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:23,264 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:23,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:23,270 : INFO : EPOCH - 2 : training on 82273 raw words (15660 effective words) took 0.2s, 92529 effective words/s\n",
      "2018-06-28 07:22:23,386 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:23,419 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:23,433 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:23,436 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:23,440 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:23,442 : INFO : EPOCH - 3 : training on 82273 raw words (15619 effective words) took 0.2s, 93201 effective words/s\n",
      "2018-06-28 07:22:23,544 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:23,581 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:23,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:23,607 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:23,610 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:23,612 : INFO : EPOCH - 4 : training on 82273 raw words (15536 effective words) took 0.2s, 92368 effective words/s\n",
      "2018-06-28 07:22:23,714 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:23,749 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 07:22:23,767 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:23,771 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:23,779 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:23,781 : INFO : EPOCH - 5 : training on 82273 raw words (15591 effective words) took 0.2s, 94552 effective words/s\n",
      "2018-06-28 07:22:23,782 : INFO : training on a 411365 raw words (77964 effective words) took 0.9s, 88325 effective words/s\n",
      "2018-06-28 07:22:23,783 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# try gensim for word2vector library\n",
    "modelTitle   = Word2Vec(title, size=1000, window=10, min_count=5, workers=5)\n",
    "modelContent = Word2Vec(content, size=1000, window=10, min_count=5, workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 07:22:24,953 : INFO : Finish creating model for title and content of paper\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Finish creating model for title and content of paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 07:22:26,785 : INFO : training model with 5 workers on 27 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-06-28 07:22:26,794 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:26,799 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:26,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:26,806 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:26,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:26,810 : INFO : EPOCH - 1 : training on 2165 raw words (368 effective words) took 0.0s, 19481 effective words/s\n",
      "2018-06-28 07:22:26,810 : WARNING : EPOCH - 1 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:26,822 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:26,825 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:26,827 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:26,830 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:26,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:26,834 : INFO : EPOCH - 2 : training on 2165 raw words (352 effective words) took 0.0s, 21853 effective words/s\n",
      "2018-06-28 07:22:26,837 : WARNING : EPOCH - 2 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:26,846 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:26,850 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:26,853 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:26,858 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:26,859 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:26,861 : INFO : EPOCH - 3 : training on 2165 raw words (375 effective words) took 0.0s, 19083 effective words/s\n",
      "2018-06-28 07:22:26,864 : WARNING : EPOCH - 3 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:26,872 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:26,874 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:26,880 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:26,882 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:26,884 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:26,884 : INFO : EPOCH - 4 : training on 2165 raw words (377 effective words) took 0.0s, 27590 effective words/s\n",
      "2018-06-28 07:22:26,886 : WARNING : EPOCH - 4 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:26,896 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:26,899 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:26,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:26,906 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:26,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:26,910 : INFO : EPOCH - 5 : training on 2165 raw words (401 effective words) took 0.0s, 22926 effective words/s\n",
      "2018-06-28 07:22:26,912 : WARNING : EPOCH - 5 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:26,920 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:26,925 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:26,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:26,928 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:26,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:26,934 : INFO : EPOCH - 6 : training on 2165 raw words (354 effective words) took 0.0s, 24585 effective words/s\n",
      "2018-06-28 07:22:26,937 : WARNING : EPOCH - 6 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:26,945 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:26,950 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:26,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:26,957 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:26,960 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:26,962 : INFO : EPOCH - 7 : training on 2165 raw words (358 effective words) took 0.0s, 18649 effective words/s\n",
      "2018-06-28 07:22:26,965 : WARNING : EPOCH - 7 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:26,974 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:26,981 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:26,982 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:26,983 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:26,986 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:26,986 : INFO : EPOCH - 8 : training on 2165 raw words (385 effective words) took 0.0s, 23630 effective words/s\n",
      "2018-06-28 07:22:26,990 : WARNING : EPOCH - 8 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:26,998 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:27,003 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:27,007 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:27,008 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:27,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:27,013 : INFO : EPOCH - 9 : training on 2165 raw words (343 effective words) took 0.0s, 17946 effective words/s\n",
      "2018-06-28 07:22:27,015 : WARNING : EPOCH - 9 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:27,027 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:27,028 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:27,036 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:27,037 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:27,038 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:27,041 : INFO : EPOCH - 10 : training on 2165 raw words (374 effective words) took 0.0s, 18452 effective words/s\n",
      "2018-06-28 07:22:27,043 : WARNING : EPOCH - 10 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:27,043 : INFO : training on a 21650 raw words (3687 effective words) took 0.3s, 14460 effective words/s\n",
      "2018-06-28 07:22:27,048 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3687, 21650)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTitle.train(title,total_examples=1000,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-28 07:22:28,713 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-06-28 07:22:28,717 : INFO : training model with 5 workers on 49 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-06-28 07:22:28,823 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:28,846 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:28,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:28,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:28,878 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:28,880 : INFO : EPOCH - 1 : training on 82273 raw words (15653 effective words) took 0.2s, 98416 effective words/s\n",
      "2018-06-28 07:22:28,882 : WARNING : EPOCH - 1 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:28,978 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:29,035 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:29,053 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:29,057 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:29,063 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:29,064 : INFO : EPOCH - 2 : training on 82273 raw words (15580 effective words) took 0.2s, 87755 effective words/s\n",
      "2018-06-28 07:22:29,065 : WARNING : EPOCH - 2 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:29,173 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:29,221 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:29,223 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:29,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:29,231 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:29,236 : INFO : EPOCH - 3 : training on 82273 raw words (15399 effective words) took 0.2s, 93237 effective words/s\n",
      "2018-06-28 07:22:29,238 : WARNING : EPOCH - 3 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:29,344 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:29,379 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:29,393 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:29,398 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:29,402 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:29,403 : INFO : EPOCH - 4 : training on 82273 raw words (15439 effective words) took 0.2s, 94732 effective words/s\n",
      "2018-06-28 07:22:29,404 : WARNING : EPOCH - 4 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:29,511 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:29,544 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:29,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:29,573 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:29,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:29,581 : INFO : EPOCH - 5 : training on 82273 raw words (15639 effective words) took 0.2s, 91502 effective words/s\n",
      "2018-06-28 07:22:29,584 : WARNING : EPOCH - 5 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:29,694 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:29,731 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:29,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:29,747 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:29,749 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:29,752 : INFO : EPOCH - 6 : training on 82273 raw words (15748 effective words) took 0.2s, 100069 effective words/s\n",
      "2018-06-28 07:22:29,754 : WARNING : EPOCH - 6 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:29,857 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:29,896 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:29,916 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:29,922 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:29,926 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:29,928 : INFO : EPOCH - 7 : training on 82273 raw words (15634 effective words) took 0.2s, 94430 effective words/s\n",
      "2018-06-28 07:22:29,931 : WARNING : EPOCH - 7 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:30,042 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:30,076 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:30,094 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:30,100 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:30,102 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:30,105 : INFO : EPOCH - 8 : training on 82273 raw words (15527 effective words) took 0.2s, 91073 effective words/s\n",
      "2018-06-28 07:22:30,106 : WARNING : EPOCH - 8 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:30,221 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:30,261 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:30,274 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:30,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:30,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:30,281 : INFO : EPOCH - 9 : training on 82273 raw words (15552 effective words) took 0.2s, 91340 effective words/s\n",
      "2018-06-28 07:22:30,282 : WARNING : EPOCH - 9 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:30,373 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-28 07:22:30,421 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-28 07:22:30,433 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 07:22:30,441 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 07:22:30,447 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 07:22:30,448 : INFO : EPOCH - 10 : training on 82273 raw words (15629 effective words) took 0.2s, 97786 effective words/s\n",
      "2018-06-28 07:22:30,453 : WARNING : EPOCH - 10 : supplied example count (32) did not equal expected count (1000)\n",
      "2018-06-28 07:22:30,458 : INFO : training on a 822730 raw words (155800 effective words) took 1.7s, 89497 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(155800, 822730)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelContent.train(content,total_examples=1000,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3619\n"
     ]
    }
   ],
   "source": [
    "model_vocab_size = len(model.wv.vocab)\n",
    "print(model_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(modelContent.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [(k, modelContent.wv[k]) for k, v in modelContent.wv.vocab.items()]\n",
    "model_vocab_list = [(k, model.wv[k]) for k, v in model.wv.vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {\"_PAD\": 0}\n",
    "\n",
    "#word2idx akan menyimpan korespondensi antara kata dan token yang dibutuhkan \n",
    "#ketika corpus ditokenisasi Embedding akan menyimpan array dari semua vectors \n",
    "#di word2vec dan akan digunakan untuk inisiasi lapisan model Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.zeros((len(modelContent.wv.vocab.items()) + 1, modelContent.vector_size))\n",
    "for i in range(len(vocab_list)):\n",
    "    word = vocab_list[i][0]\n",
    "    word2idx[word] = i + 1\n",
    "    embeddings_matrix[i+1] = vocab_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings_matrix = np.zeros((len(model.wv.vocab.items()) + 1, model.vector_size))\n",
    "for i in range(len(vocab_list)):\n",
    "    word = vocab_list[i][0]\n",
    "    word2idx[word] = i + 1\n",
    "    model_embeddings_matrix[i+1] = vocab_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.02900526 -0.03835749 -0.03446946 ... -0.05384898  0.02977691\n",
      "  -0.01353927]\n",
      " [ 0.06862338 -0.0326765  -0.11657048 ... -0.0775702   0.06904967\n",
      "   0.02572765]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(model_embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "EMBEDDING_DIM = 100 #dimensi dari word vector\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings_matrix),\n",
    "                           EMBEDDING_DIM,\n",
    "                           weights=[embeddings_matrix],\n",
    "                           trainable= False)\n",
    "model_embedding_layer = Embedding(len(model_embeddings_matrix),\n",
    "                                  EMBEDDING_DIM,\n",
    "                                  weights=[model_embeddings_matrix],\n",
    "                                  trainable= False\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Untuk visualisasi kita menggunakan visual tool bernama TensorBoard, model pada Keras sangat\n",
    "#penting untuk menyimpan progres latihan ke Tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "logdir = \"./logs/{}\".format('%(asctime)s : %(levelname)s : %(message)s')\n",
    "tensorBoard = TensorBoard(log_dir = logdir,\n",
    "                          histogram_freq = 1,\n",
    "                          write_graph = True,\n",
    "                          write_images = False,\n",
    "                          embeddings_freq = 1,\n",
    "                          embeddings_layer_names = None,\n",
    "                          embeddings_metadata = None,\n",
    "                         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setelah kode diatas dibuat, jalankan tensorboard dengan perintah:\n",
    "# `tensorboard --logdir='/.logs/'\n",
    "# Atau gunakan kode dibawah ini untuk melihat kata-kata dan tidaklah id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file = \"w2v_metadata.tsv\"\n",
    "#sort based on id\n",
    "word2idx_sorted = [(k,word2idx[k]) for k in sorted(word2idx,key=word2idx.get, reverse = False)]\n",
    "\n",
    "logdir = \"./logs/{}\".format('./')\n",
    "file_metadata = open(os.path.join(logdir, meta_file), 'w+')\n",
    "try:\n",
    "    for word in word2idx_sorted:\n",
    "        if word[0] =='':\n",
    "            print(\"baris kosong harus dihapus atau akan terdapat bug\")\n",
    "            file_metadata.write('<Baris Kosong>' + '\\n')\n",
    "        else:\n",
    "            file_metadata.write(word[0]+'\\n')\n",
    "finally:\n",
    "    file_metadata.close()\n",
    "\n",
    "tensorBoard = TensorBoard(log_dir = logdir,\n",
    "                          histogram_freq = 1,\n",
    "                          write_graph = True,\n",
    "                          write_images = False,\n",
    "                          embeddings_freq = 1,\n",
    "                          embeddings_layer_names = None,\n",
    "                          embeddings_metadata = meta_file) #inisialisasi tensorboard dengan meta data\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atau dapat juga menggunakan script yang telah dibuat sebagai berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-89-1c3c640742cd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-89-1c3c640742cd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python3 w2v_visualizer.py './file.json' './logs'\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python3 w2v_visualizer.py './file.json' './logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
